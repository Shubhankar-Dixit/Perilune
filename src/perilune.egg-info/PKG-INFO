Metadata-Version: 2.4
Name: perilune
Version: 0.0.1
Summary: Perilune backend scaffolding for exoplanet discovery
Author: Perilune Team
License: MIT
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi<1,>=0.111
Requires-Dist: uvicorn[standard]<1,>=0.30
Requires-Dist: pydantic<3,>=2.6
Requires-Dist: numpy<2,>=1.26
Requires-Dist: pandas<3,>=2.2
Requires-Dist: httpx<1,>=0.27
Requires-Dist: pyarrow>=21.0.0
Requires-Dist: fastparquet>=2024.11.0
Provides-Extra: astro
Requires-Dist: lightkurve<3,>=2.4; extra == "astro"
Requires-Dist: astropy<7,>=6; extra == "astro"
Provides-Extra: ml
Requires-Dist: scikit-learn<2,>=1.4; extra == "ml"
Requires-Dist: xgboost<3,>=2.0; extra == "ml"
Requires-Dist: lightgbm<5,>=4.3; extra == "ml"
Provides-Extra: dev
Requires-Dist: pytest<9,>=8; extra == "dev"
Requires-Dist: ruff<1,>=0.5; extra == "dev"

# Perilune

End-to-end exoplanet transit discovery and explanation on open NASA data.

- Start with the project overview in `Info.md`.
- See the technical plan and API contracts in `docs/plan.md`.
- Contributor guidelines live in `AGENTS.md`.

## Getting Data

1. Install dependencies (including astronomy extras): `uv sync --extra astro`.
2. Fetch a mission catalog (example downloads the first 5k KOI rows to `data/raw/koi.csv`):
   `uv run python -m src.pipelines.fetch_data --table koi --limit 5000`
3. Download and cache a light curve with Lightkurve (writes JSON under `data/raw/lightcurves/`):
   `uv run python -m src.pipelines.lightcurves KOI-0001 --mission kepler`
4. (Optional) Once the BLS implementation is swapped from the placeholder, call `/api/predict` with `dryRun=false` to exercise the full pipeline.
5. Harmonise the catalogues into `data/processed/catalog_merged.csv`:
   `uv run python -m src.pipelines.catalogs --output data/processed/catalog_merged.csv`
6. Build the modeling feature table (optionally pointing to a directory of BLS JSON outputs):
   `uv run python -m src.pipelines.features --catalog data/processed/catalog_merged.csv --output data/processed/features.parquet`
7. Train the baseline model (requires `uv sync --extra ml` once):
   `uv run python -m src.models.baseline --features data/processed/features.parquet --output-dir artifacts/baseline`
   - The API automatically looks for artifacts under `artifacts/baseline` (override with `PERILUNE_BASELINE_DIR`).
